{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward-Only Token-Adaptive Learning (FOTAL) Demo\n",
    "\n",
    "This notebook demonstrates the capabilities of the NoBackdrop model, which implements Forward-Only Token-Adaptive Learning (FOTAL). This novel approach enables efficient language model training without backpropagation, making it suitable for training on limited hardware such as an RTX 3050 GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "from no_backdrop.model.hebbian_lm import HebbianLM\n",
    "from no_backdrop.training.trainer import Trainer\n",
    "from no_backdrop.training.data_utils import prepare_dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Creating a FOTAL Model\n",
    "\n",
    "First, let's create a FOTAL model with appropriate parameters for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Ensure the tokenizer has padding token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Create model\n",
    "model = HebbianLM(\n",
    "    vocab_size=len(tokenizer),\n",
    "    hidden_size=256,  # Small size for demonstration\n",
    "    num_hidden_layers=4,\n",
    "    num_attention_heads=8,\n",
    "    window_size=64,\n",
    "    dropout=0.1,\n",
    "    max_position_embeddings=512,\n",
    "    update_rate=0.01,\n",
    "    use_fast_weights=True,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    bos_token_id=tokenizer.bos_token_id if tokenizer.bos_token_id is not None else tokenizer.cls_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id if tokenizer.eos_token_id is not None else tokenizer.sep_token_id,\n",
    ")\n",
    "\n",
    "# Move model to device\n",
    "model.to(device)\n",
    "\n",
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    device=device,\n",
    "    log_interval=10,\n",
    "    eval_interval=50,\n",
    "    save_interval=100,\n",
    "    checkpoint_dir=\"./checkpoints\",\n",
    "    use_wandb=False,\n",
    ")\n",
    "\n",
    "print(f\"Model created with {sum(p.numel() for p in model.parameters())} parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training on a Small Dataset\n",
    "\n",
    "Let's train our model on a small dataset to demonstrate the forward-only learning approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load a small dataset\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "\n",
    "# Extract a small subset for demonstration\n",
    "train_texts = dataset[\"train\"][\"text\"][:100]  # Just 100 examples\n",
    "eval_texts = dataset[\"validation\"][\"text\"][:20]  # Just 20 examples\n",
    "\n",
    "# Filter out empty texts\n",
    "train_texts = [text for text in train_texts if text.strip()]\n",
    "eval_texts = [text for text in eval_texts if text.strip()]\n",
    "\n",
    "print(f\"Training on {len(train_texts)} texts, evaluating on {len(eval_texts)} texts\")\n",
    "\n",
    "# Prepare dataloaders\n",
    "train_dataloader, eval_dataloader = prepare_dataloaders(\n",
    "    train_texts=train_texts,\n",
    "    eval_texts=eval_texts,\n",
    "    tokenizer=tokenizer,\n",
    "    batch_size=2,  # Small batch size for demonstration\n",
    "    max_length=128,  # Short sequences for demonstration\n",
    "    stride=64,\n",
    "    num_workers=0,  # Use 0 for Jupyter notebook\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Train for a few steps\n",
    "history = trainer.train(\n",
    "    train_dataloader=train_dataloader,\n",
    "    eval_dataloader=eval_dataloader,\n",
    "    num_epochs=1,\n",
    "    max_steps=50,  # Just 50 steps for demonstration\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualizing Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot training and evaluation loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history[\"train_loss\"], label=\"Train Loss\")\n",
    "plt.plot([i * trainer.eval_interval for i in range(len(history[\"eval_loss\"]))], \n",
    "         history[\"eval_loss\"], label=\"Eval Loss\", marker=\"o\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training and Evaluation Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot perplexity\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history[\"train_perplexity\"], label=\"Train Perplexity\")\n",
    "plt.plot([i * trainer.eval_interval for i in range(len(history[\"eval_perplexity\"]))], \n",
    "         history[\"eval_perplexity\"], label=\"Eval Perplexity\", marker=\"o\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"Perplexity\")\n",
    "plt.title(\"Training and Evaluation Perplexity\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Text Generation\n",
    "\n",
    "Let's generate some text with our trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Define prompts\n",
    "prompts = [\n",
    "    \"Once upon a time\",\n",
    "    \"The meaning of life is\",\n",
    "    \"In the future, artificial intelligence will\",\n",
    "]\n",
    "\n",
    "# Generate text\n",
    "for prompt in prompts:\n",
    "    generated_texts = trainer.generate_text(\n",
    "        prompt=prompt,\n",
    "        tokenizer=tokenizer,\n",
    "        max_length=50,\n",
    "        temperature=0.7,\n",
    "        top_k=50,\n",
    "        top_p=0.9,\n",
    "        update_model=False,  # Don't update model during generation\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nPrompt: {prompt}\")\n",
    "    print(f\"Generated: {generated_texts[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Streaming Adaptation\n",
    "\n",
    "One of the key features of FOTAL is its ability to adapt during inference. Let's demonstrate this capability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Define a prompt\n",
    "prompt = \"The capital of France is\"\n",
    "\n",
    "# Generate text before adaptation\n",
    "print(\"Before adaptation:\")\n",
    "generated_texts = trainer.generate_text(\n",
    "    prompt=prompt,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=30,\n",
    "    temperature=0.7,\n",
    "    top_k=50,\n",
    "    top_p=0.9,\n",
    "    update_model=False,\n",
    ")\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"Generated: {generated_texts[0]}\")\n",
    "\n",
    "# Adapt the model with new information\n",
    "adaptation_text = \"The capital of France is Paris. The capital of Italy is Rome. The capital of Spain is Madrid.\"\n",
    "print(f\"\\nAdapting model with: {adaptation_text}\")\n",
    "\n",
    "# Tokenize adaptation text\n",
    "tokenized = tokenizer(adaptation_text, return_tensors=\"pt\")\n",
    "adaptation_ids = tokenized[\"input_ids\"].to(device)\n",
    "attention_mask = torch.ones_like(adaptation_ids)\n",
    "\n",
    "# Forward pass with adaptation\n",
    "with torch.no_grad():\n",
    "    outputs = model(\n",
    "        input_ids=adaptation_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        update_model=True,  # Enable adaptation\n",
    "        compute_loss=False,\n",
    "    )\n",
    "\n",
    "# Generate text after adaptation\n",
    "print(\"\\nAfter adaptation:\")\n",
    "generated_texts = trainer.generate_text(\n",
    "    prompt=prompt,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=30,\n",
    "    temperature=0.7,\n",
    "    top_k=50,\n",
    "    top_p=0.9,\n",
    "    update_model=False,\n",
    ")\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"Generated: {generated_texts[0]}\")\n",
    "\n",
    "# Try with a different prompt\n",
    "new_prompt = \"The capital of Italy is\"\n",
    "generated_texts = trainer.generate_text(\n",
    "    prompt=new_prompt,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=30,\n",
    "    temperature=0.7,\n",
    "    top_k=50,\n",
    "    top_p=0.9,\n",
    "    update_model=False,\n",
    ")\n",
    "print(f\"\\nPrompt: {new_prompt}\")\n",
    "print(f\"Generated: {generated_texts[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Single Batch Learning\n",
    "\n",
    "FOTAL models can learn effectively from a single batch. Let's demonstrate this capability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create a new model for this demonstration\n",
    "new_model = HebbianLM(\n",
    "    vocab_size=len(tokenizer),\n",
    "    hidden_size=256,\n",
    "    num_hidden_layers=4,\n",
    "    num_attention_heads=8,\n",
    "    window_size=64,\n",
    "    dropout=0.1,\n",
    "    max_position_embeddings=512,\n",
    "    update_rate=0.05,  # Higher update rate for faster learning\n",
    "    use_fast_weights=True,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    bos_token_id=tokenizer.bos_token_id if tokenizer.bos_token_id is not None else tokenizer.cls_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id if tokenizer.eos_token_id is not None else tokenizer.sep_token_id,\n",
    ")\n",
    "\n",
    "# Move model to device\n",
    "new_model.to(device)\n",
    "\n",
    "# Create trainer\n",
    "new_trainer = Trainer(\n",
    "    model=new_model,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# Define a text to learn\n",
    "learning_text = \"\"\"\n",
    "The quick brown fox jumps over the lazy dog. \n",
    "Python is a high-level, interpreted programming language. \n",
    "Machine learning is a field of study that gives computers the ability to learn without being explicitly programmed.\n",
    "\"\"\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokenized = tokenizer(learning_text, return_tensors=\"pt\")\n",
    "input_ids = tokenized[\"input_ids\"].to(device)\n",
    "attention_mask = torch.ones_like(input_ids)\n",
    "\n",
    "# Prepare batch\n",
    "batch = {\n",
    "    \"input_ids\": input_ids,\n",
    "    \"attention_mask\": attention_mask,\n",
    "}\n",
    "\n",
    "# Measure initial perplexity\n",
    "new_model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = new_model(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        update_model=False,\n",
    "        compute_loss=True,\n",
    "    )\n",
    "\n",
    "initial_loss = outputs[\"loss\"].item()\n",
    "initial_perplexity = new_trainer._compute_perplexity(outputs[\"loss\"])\n",
    "\n",
    "print(f\"Initial loss: {initial_loss:.4f}\")\n",
    "print(f\"Initial perplexity: {initial_perplexity:.2f}\")\n",
    "\n",
    "# Train on a single batch\n",
    "print(\"\\nTraining on a single batch...\")\n",
    "metrics = new_trainer.train_step(batch)\n",
    "\n",
    "# Measure final perplexity\n",
    "new_model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = new_model(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        update_model=False,\n",
    "        compute_loss=True,\n",
    "    )\n",
    "\n",
    "final_loss = outputs[\"loss\"].item()\n",
    "final_perplexity = new_trainer._compute_perplexity(outputs[\"loss\"])\n",
    "\n",
    "print(f\"Final loss: {final_loss:.4f}\")\n",
    "print(f\"Final perplexity: {final_perplexity:.2f}\")\n",
    "print(f\"Improvement: {initial_perplexity - final_perplexity:.2f}\")\n",
    "\n",
    "# Generate text based on the learned content\n",
    "prompts = [\n",
    "    \"The quick brown\",\n",
    "    \"Python is a\",\n",
    "    \"Machine learning is\",\n",
    "]\n",
    "\n",
    "print(\"\\nGenerating text based on learned content:\")\n",
    "for prompt in prompts:\n",
    "    generated_texts = new_trainer.generate_text(\n",
    "        prompt=prompt,\n",
    "        tokenizer=tokenizer,\n",
    "        max_length=30,\n",
    "        temperature=0.7,\n",
    "        top_k=50,\n",
    "        top_p=0.9,\n",
    "        update_model=False,\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nPrompt: {prompt}\")\n",
    "    print(f\"Generated: {generated_texts[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Memory Usage Analysis\n",
    "\n",
    "Let's analyze the memory usage of our FOTAL model compared to traditional models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create a baseline model for comparison\n",
    "from transformers import GPT2Config, GPT2LMHeadModel\n",
    "\n",
    "# Create a GPT-2 model with similar size\n",
    "config = GPT2Config(\n",
    "    vocab_size=len(tokenizer),\n",
    "    n_positions=512,\n",
    "    n_ctx=512,\n",
    "    n_embd=256,\n",
    "    n_layer=4,\n",
    "    n_head=8,\n",
    ")\n",
    "\n",
    "baseline_model = GPT2LMHeadModel(config)\n",
    "baseline_model.to(device)\n",
    "\n",
    "# Measure memory usage for NoBackdrop model\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Generate random input\n",
    "input_ids = torch.randint(0, model.vocab_size, (1, 512), device=device)\n",
    "attention_mask = torch.ones_like(input_ids)\n",
    "\n",
    "# Forward pass with NoBackdrop model\n",
    "model(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    update_model=True,\n",
    "    compute_loss=True,\n",
    ")\n",
    "\n",
    "no_backdrop_memory = torch.cuda.max_memory_allocated() / (1024 ** 2)  # MB\n",
    "\n",
    "# Measure memory usage for baseline model\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Forward pass with baseline model\n",
    "baseline_model(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    labels=input_ids,\n",
    ")\n",
    "\n",
    "baseline_memory = torch.cuda.max_memory_allocated() / (1024 ** 2)  # MB\n",
    "\n",
    "# Calculate memory ratio\n",
    "memory_ratio = no_backdrop_memory / baseline_memory if baseline_memory > 0 else float('inf')\n",
    "\n",
    "print(f\"NoBackdrop Memory Usage: {no_backdrop_memory:.2f} MB\")\n",
    "print(f\"Baseline Memory Usage: {baseline_memory:.2f} MB\")\n",
    "print(f\"Memory Ratio: {memory_ratio:.2f}x\")\n",
    "\n",
    "# Plot memory usage\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar([\"NoBackdrop\", \"Baseline\"], [no_backdrop_memory, baseline_memory])\n",
    "plt.title(\"Memory Usage Comparison\")\n",
    "plt.ylabel(\"Memory Usage (MB)\")\n",
    "plt.grid(True, axis='y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training Speed Analysis\n",
    "\n",
    "Let's compare the training speed of our FOTAL model with traditional backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import time\n",
    "\n",
    "# Generate random input\n",
    "batch_size = 4\n",
    "seq_len = 128\n",
    "input_ids = torch.randint(0, model.vocab_size, (batch_size, seq_len), device=device)\n",
    "attention_mask = torch.ones_like(input_ids)\n",
    "\n",
    "# Prepare batch\n",
    "batch = {\n",
    "    \"input_ids\": input_ids,\n",
    "    \"attention_mask\": attention_mask,\n",
    "}\n",
    "\n",
    "# Benchmark NoBackdrop model\n",
    "num_steps = 10\n",
    "model.train()\n",
    "torch.cuda.synchronize()\n",
    "start_time = time.time()\n",
    "\n",
    "for _ in range(num_steps):\n",
    "    trainer.train_step(batch)\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "no_backdrop_time = time.time() - start_time\n",
    "\n",
    "# Benchmark baseline model\n",
    "baseline_model.train()\n",
    "optimizer = torch.optim.AdamW(baseline_model.parameters(), lr=5e-5)\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "start_time = time.time()\n",
    "\n",
    "for _ in range(num_steps):\n",
    "    # Forward pass\n",
    "    outputs = baseline_model(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        labels=input_ids,\n",
    "    )\n",
    "    \n",
    "    # Backward pass\n",
    "    loss = outputs.loss\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update parameters\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "baseline_time = time.time() - start_time\n",
    "\n",
    "# Calculate speed ratio\n",
    "speed_ratio = baseline_time / no_backdrop_time if no_backdrop_time > 0 else float('inf')\n",
    "\n",
    "print(f\"NoBackdrop Training Time: {no_backdrop_time:.4f} seconds\")\n",
    "print(f\"Baseline Training Time: {baseline_time:.4f} seconds\")\n",
    "print(f\"Speed Ratio: {speed_ratio:.2f}x\")\n",
    "print(f\"NoBackdrop Steps per Second: {num_steps / no_backdrop_time:.2f}\")\n",
    "print(f\"Baseline Steps per Second: {num_steps / baseline_time:.2f}\")\n",
    "\n",
    "# Plot training speed\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar([\"NoBackdrop\", \"Baseline\"], [num_steps / no_backdrop_time, num_steps / baseline_time])\n",
    "plt.title(\"Training Speed Comparison\")\n",
    "plt.ylabel(\"Steps per Second\")\n",
    "plt.grid(True, axis='y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Conclusion\n",
    "\n",
    "In this notebook, we've demonstrated the key features of the NoBackdrop model with Forward-Only Token-Adaptive Learning (FOTAL):\n",
    "\n",
    "1. **Efficient Training**: The model can be trained without backpropagation, making it suitable for limited hardware.\n",
    "2. **Streaming Adaptation**: The model can adapt to new information during inference.\n",
    "3. **Single Batch Learning**: The model can learn effectively from a single batch of data.\n",
    "4. **Memory Efficiency**: The model uses less memory compared to traditional backpropagation models.\n",
    "5. **Training Speed**: The model trains faster than traditional backpropagation models.\n",
    "\n",
    "These features make FOTAL a promising approach for efficient language model training and adaptation on limited hardware."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
